# llm-ml-rag-evaluation


This project is a framework that evaluates how **Large Language Models (LLMs)** perform when they are used to predict, explain, and generate counterfactuals for traditional **Machine Learning (ML) models**. The main goal is to understand whether LLMs can reliably interpret ML model behavior when they are given proper data context.

The framework compares:
- Predictions from ML models  
- Predictions and explanations generated by LLMs  
- Results from standard baseline explanation methods  

All comparisons are done in a structured and repeatable way.


**Motivation**

Machine Learning models often work well but are hard to explain. LLMs have strong reasoning and language abilities, but their reliability on structured ML tasks is still unclear. This project helps study:
- Whether LLM explanations align with ML model behavior  
- How consistent LLM predictions are compared to ML models  
- How LLM-based explanations compare with classical methods like LIME and DiCE  


**How the Framework Works**

1. Pre-trained ML models and datasets are loaded  
2. Test data points are selected either randomly or using distance-based strategies  
3. Training data is clustered to group similar samples  
4. Relevant clusters are retrieved and used as context (RAG) for LLM prompts  
5. Outputs are generated by ML models, LLMs, and baseline methods  
6. Results are evaluated and saved  


**Key Features**

- Regression and classification support  
- Clustering-based Retrieval Augmented Generation (RAG)  
- LLM predictions, explanations, and counterfactuals  
- Baseline methods such as LIME and DiCE  
- Config-driven experiments  
- Automatic evaluation and visualization  


## Workflow Overview:

This project follows a step-by-step evaluation pipeline to compare Large Language Models (LLMs) with traditional Machine Learning (ML) models.

1. **Configuration Loading:**  
   Experiment settings are loaded from a YAML configuration file. This defines the ML models, LLMs, metrics, test size, clustering strategy, and output settings.

2. **Model and Data Loading:**  
   Pre-trained ML models along with their training and test datasets are loaded based on the configuration.

3. **Test Data Selection:**  
   Test samples are selected either randomly or using distance-based strategies to include both typical and challenging data points.

4. **Clustering and Retrieval (RAG):**  
   Training data is clustered to group similar samples. Relevant clusters are retrieved and used as structured context for the LLM prompts.

5. **LLM Prompting:**  
   Prompts are constructed using test data and retrieved cluster context, then sent to one or more LLMs for prediction or explanation.

6. **Baseline Method Execution:**  
   Baseline explainability methods such as LIME (for predictions and feature importance) and DiCE (for counterfactuals) are executed for comparison.

7. **Evaluation:**  
   Outputs from the LLMs, ML models, and baseline methods are evaluated using selected metrics to measure accuracy and explanation quality.

8. **Result Storage and Visualization:**  
   Evaluation results, raw outputs, prompts, and comparison plots are automatically saved for analysis.
