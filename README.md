# llm-ml-rag-evaluation

## Overview

This project is a framework that evaluates how **Large Language Models (LLMs)** perform when they are used to predict, explain, and generate counterfactuals for traditional **Machine Learning (ML) models**. The main goal is to understand whether LLMs can reliably interpret ML model behavior when they are given proper data context.

The framework compares:
- Predictions from ML models  
- Predictions and explanations generated by LLMs  
- Results from standard baseline explanation methods  

All comparisons are done in a structured and repeatable way.



## Motivation

Machine Learning models often work well but are hard to explain. LLMs have strong reasoning and language abilities, but their reliability on structured ML tasks is still unclear. This project helps study:
- Whether LLM explanations align with ML model behavior  
- How consistent LLM predictions are compared to ML models  
- How LLM-based explanations compare with classical methods like LIME and DiCE  

---

## How the Framework Works

1. Pre-trained ML models and datasets are loaded.  
2. Test data points are selected either randomly or using distance-based strategies.  
3. Training data is clustered to group similar samples.  
4. Relevant clusters are retrieved and used as context (RAG) for LLM prompts.  
5. For each test point, outputs are generated by:
   - The ML model itself  
   - One or more LLMs  
   - Baseline explanation methods  
6. All outputs are evaluated and compared using selected metrics.  
7. Results, prompts, and plots are saved for analysis.

---

## Key Features

- Supports **regression and classification** tasks  
- Uses **clustering-based Retrieval Augmented Generation (RAG)**  
- Compares LLM outputs with ML model predictions  
- Includes baseline methods such as **LIME** and **DiCE**  
- Fully **config-driven** using a YAML file  
- Automatic evaluation, logging, and visualization  

---

## Project Structure

```text
.
├── config.yml                  # Experiment configuration
├── main.py                     # Main execution script
├── Main_modules.py             # Core ML, LLM, and evaluation logic
├── Clustering_classes.py       # Clustering and RAG components
├── Plotting_functions.py       # Visualization utilities
├── results/                    # Saved results and plots
├── README.md
└── LICENSE
