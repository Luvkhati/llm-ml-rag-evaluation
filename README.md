# LLM-ML-RAG-Evaluation


This project is a framework that evaluates how **Large Language Models (LLMs)** perform when they are used to predict, explain, and generate counterfactuals for traditional **Machine Learning (ML) models**. The main goal is to understand whether LLMs can reliably interpret ML model behavior when they are given proper data context.

The framework compares:
- Predictions from ML models  
- Predictions and explanations generated by LLMs  
- Results from standard baseline explanation methods  

All comparisons are done in a structured and repeatable way.


**Motivation:**

Machine Learning models often achieve strong performance but remain difficult to trust in high-stakes domains
(e.g., healthcare, networking, and environmental monitoring) because their decisions are hard to explain.

Traditional XAI methods such as LIME, SHAP, and DiCE commonly rely on perturbation-based synthetic samples,
which can produce explanations that are invalid, unrealistic, or context-agnostic.

LLMs can generate natural-language explanations and support interactive reasoning, but their reliability
depends strongly on being grounded in real evidence rather than reasoning without context. This project helps study:

- Whether evidence-grounded LLM explanations remain faithful to black-box ML model behavior  
- How retrieval of local training regimes (Mixture-of-Experts) improves explanation context-awareness and trustability  
- How the proposed ELMoE framework compares with classical baselines such as LIME (feature attribution)
  and DiCE (counterfactuals) in terms of explanation quality and robustness  
  

**How the Framework Works:**

1. Pre-trained ML models and datasets are loaded  
2. Test data points are selected either randomly or using distance-based strategies  
3. Training data is clustered to group similar samples  
4. Relevant clusters are retrieved and used as context (RAG) for LLM prompts  
5. Outputs are generated by ML models, LLMs, and baseline methods  
6. Results are evaluated and saved  


**Key Features:**

- Regression and classification support  
- Clustering-based Retrieval Augmented Generation (RAG)  
- LLM predictions, explanations, and counterfactuals  
- Baseline methods such as LIME and DiCE  
- Config-driven experiments  
- Automatic evaluation and visualization  


## Workflow Overview:

This project follows a step-by-step evaluation pipeline to compare Large Language Models (LLMs) with traditional Machine Learning (ML) models.

1. **Configuration Loading:**  
   Experiment settings are loaded from a YAML configuration file. This defines the ML models, LLMs, metrics, test size, clustering strategy, and output settings.

2. **Model and Data Loading:**  
   Pre-trained ML models along with their training and test datasets are loaded based on the configuration.

3. **Test Data Selection:**  
   Test samples are selected either randomly or using distance-based strategies to include both typical and challenging data points.

4. **Clustering and Retrieval (RAG):**  
   Training data is clustered to group similar samples. Relevant clusters are retrieved and used as structured context for the LLM prompts.

5. **LLM Prompting:**  
   Prompts are constructed using test data and retrieved cluster context, then sent to one or more LLMs for prediction or explanation.

6. **Baseline Method Execution:**  
   Baseline explainability methods such as LIME (for predictions and feature importance) and DiCE (for counterfactuals) are executed for comparison.

7. **Evaluation:**  
   Outputs from the LLMs, ML models, and baseline methods are evaluated using selected metrics to measure accuracy and explanation quality.

8. **Result Storage and Visualization:**  
   Evaluation results, raw outputs, prompts, and comparison plots are automatically saved for analysis.
